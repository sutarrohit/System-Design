# **Back Pressure**

Back pressure is a **flow control mechanism** used in **asynchronous systems, message queues, and APIs** to prevent **overloading a system** when it cannot handle incoming requests quickly enough.

Without back pressure, an overloaded system might:  
❌ **Consume too much memory** (causing crashes)  
❌ **Slow down due to excessive disk reads/writes**  
❌ **Cause queue overflow, leading to lost or delayed messages**

### **Analogy**

Imagine a **restaurant kitchen** where **orders (requests) come in faster than chefs can cook**.

-   If orders keep piling up, the kitchen **gets overwhelmed** and food quality drops.
-   The restaurant may **temporarily stop taking orders** (back pressure) to **prevent overload**.
-   Customers may be asked to **wait or come back later** (retry strategy).

---

## **How Back Pressure Works in a System**

### **1. Monitor Queue Size**

-   If the message queue **starts filling up**, apply back pressure to slow down new incoming requests.

### **2. Reject or Delay New Requests**

-   If the queue **exceeds a threshold**, **reject** new requests (return HTTP 503) or **delay processing** with exponential backoff.

### **3. Shed Load (Drop Oldest or Least Important Jobs)**

-   Drop non-essential tasks to **prioritize critical ones** (e.g., prioritize payment processing over analytics).

### **4. Scale Up Resources**

-   If possible, dynamically **add more workers** to process requests faster.

---

## **Example: Implementing Back Pressure in Node.js with BullMQ (Redis-based Queue System)**

### **Scenario:**

We are processing **video uploads** in a queue. If the queue **grows too large**, we apply back pressure by **rejecting new uploads (HTTP 503) or delaying them with exponential backoff**.

### **Implementation Steps:**

1. **Create a queue with a max size**
2. **Reject new jobs if queue is full** (return `503 Service Unavailable`)
3. **Retry with exponential backoff** if rejected

---

### **Step 1: Setup BullMQ Queue**

```javascript
const Queue = require("bullmq").Queue;
const videoQueue = new Queue("video-processing", {
    connection: { host: "localhost", port: 6379 }
});

// Define max queue size
const MAX_QUEUE_SIZE = 50;
```

---

### **Step 2: Add Job with Back Pressure Handling**

```javascript
const addVideoJob = async (videoId) => {
    // Check queue size
    const jobCounts = await videoQueue.getJobCounts();

    if (jobCounts.waiting >= MAX_QUEUE_SIZE) {
        throw new Error("Server Busy: Too many requests. Try again later.");
    }

    // Add job to queue
    await videoQueue.add(
        "process-video",
        { videoId },
        {
            attempts: 5, // Retry up to 5 times
            backoff: { type: "exponential", delay: 1000 } // Exponential backoff
        }
    );

    console.log(`Job added: ${videoId}`);
};
```

---

### **Step 3: Handle API Requests with Back Pressure**

```javascript
const express = require("express");
const app = express();

app.post("/upload-video", async (req, res) => {
    try {
        const { videoId } = req.body;
        await addVideoJob(videoId);
        res.status(202).json({ message: "Video processing started" });
    } catch (error) {
        res.status(503).json({ error: "Server too busy, try again later" });
    }
});

app.listen(3000, () => console.log("Server running on port 3000"));
```

---

### **Step 4: Process Jobs in the Queue**

```javascript
const { Worker } = require("bullmq");

const videoWorker = new Worker("video-processing", async (job) => {
    console.log(`Processing video: ${job.data.videoId}`);

    // Simulate video processing delay
    await new Promise((resolve) => setTimeout(resolve, 5000));

    console.log(`Video processed: ${job.data.videoId}`);
});
```

---

## **Back Pressure Strategies**

### ✅ **1. Reject Requests When Queue is Full (HTTP 503)**

-   If the queue reaches the max limit, **return HTTP 503** (Service Unavailable).
-   Clients **must retry later** (possibly using **exponential backoff**).

### ✅ **2. Exponential Backoff Retries**

Instead of retrying immediately, clients retry with increasing delays:

```
1st retry: Wait 1 sec
2nd retry: Wait 2 sec
3rd retry: Wait 4 sec
4th retry: Wait 8 sec
```

**Example: Retry Request in Client Code (JavaScript)**

```javascript
const fetchWithRetry = async (url, retries = 5, delay = 1000) => {
    for (let i = 0; i < retries; i++) {
        try {
            let response = await fetch(url, { method: "POST" });
            if (response.status === 503) throw new Error("Server busy");
            return await response.json();
        } catch (error) {
            console.log(`Retry ${i + 1}/${retries} in ${delay}ms`);
            await new Promise((res) => setTimeout(res, delay));
            delay *= 2; // Exponential backoff
        }
    }
    throw new Error("Failed after retries");
};

fetchWithRetry("/upload-video");
```

---

## **Handling Back Pressure in Different Systems**

### 🔹 **Message Queues (RabbitMQ, Kafka, BullMQ)**

-   **Limit max queue size**
-   **Drop old messages** if queue is full
-   **Increase consumers/workers** dynamically

### 🔹 **APIs (REST, GraphQL, gRPC)**

-   **Use rate limiting** (e.g., API Gateway, Nginx)
-   **Return HTTP 503** when overloaded
-   **Client retries with exponential backoff**

### 🔹 **Databases (PostgreSQL, MySQL, MongoDB)**

-   **Use connection pooling** to limit concurrent connections
-   **Queue slow queries** instead of running everything at once
-   **Implement read replicas** to distribute load

---

## **Conclusion**

✅ **Back pressure prevents system overload and crashes.**  
✅ **Rejecting or delaying requests helps maintain performance.**  
✅ **Exponential backoff ensures smoother retries.**  
✅ **Scaling resources dynamically can reduce pressure.**
