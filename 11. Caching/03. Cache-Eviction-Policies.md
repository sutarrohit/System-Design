# Cache Eviction Policies

The process of clearing data from a cache to create space for fresh or more relevant information is known as cache eviction. It enhances system speed by caching and storing frequently accessed data for faster retrieval. Caches have a limited capacity, though, and the system must choose which data to delete when the cache is full. The cache eviction policies provide the criteria for choosing which data to replace.

## Cache Eviction Policies

-   **Least Recently Used (LRU)**
-   **Least Frequently Used (LFU)**
-   **First-In-First-Out (FIFO)**
-   **Random Replacement**

## What are Cache Eviction Policies?

Algorithms or techniques known as cache eviction policies decide which data should be deleted from a cache when its capacity is reached. By keeping the most relevant and often requested data in the cache, these strategies seek to optimize its effectiveness. In systems with limited cache space, effective cache eviction procedures are essential for preserving peak performance while guaranteeing that important data is kept for easy access.

---

## 1. Least Recently Used (LRU)

When the cache hits its capacity limit, the Least Recently Used (LRU) cache eviction policy is designed to eliminate the item that has been accessed the least recently. Items that have not been accessed for a longer period of time are assumed to be less likely to be used in the near future. When the cache is full, LRU evicts the item that hasn't been accessed in the longest time since it keeps track of the order in which items are accessed.

### Example:

Let us consider a cache with a maximum capacity of 3, initially containing items A, B, and C in that order.

-   If a new item, D, is accessed, the cache becomes full, and the LRU policy would evict the least recently used item, which is A. The cache now holds items B, C, and D.
-   If item B is accessed next, the order becomes C, D, B.
-   If another item, E, is accessed, the cache is full again, and the LRU policy would evict C, resulting in the cache holding items D, B, and E.

![](https://media.geeksforgeeks.org/wp-content/uploads/20240110185803/Least-Recently-Used.jpg)

### Advantages of LRU

-   **Easy Implementation:** LRU is simple to understand and implement.
-   **Efficient Use of Cache:** Ensures frequently accessed items remain in cache.
-   **Adaptability:** Works well for databases, web caching, and file systems.

### Disadvantages of LRU

-   **Strict Ordering:** Assumes past access predicts future relevance.
-   **Cold Start Issues:** Requires historical data for optimal performance.
-   **Memory Overhead:** Requires tracking access order, increasing memory usage.

### Use Cases of LRU

-   **Web Caching:** Stores frequently accessed web resources.
-   **Database Management:** Caches query results for faster retrieval.
-   **File Systems:** Keeps frequently accessed files and directories in memory.

---

---

## 2. Least Frequently Used (LFU)

The least frequently accessed entries are eliminated first under the LFU cache eviction policy. It is based on the idea that things that are used the least are less likely to be needed later. When the cache is full, LFU removes the item with the lowest access frequency after keeping track of the amount of times each item is accessed.

### Example:

Consider a cache with items X, Y, and Z. If item Z has been accessed fewer times than items X and Y, the LFU policy will retain the items X and Y and potentially evict item Z when the cache reaches its capacity.

![](https://media.geeksforgeeks.org/wp-content/uploads/20240110185835/Least-Frequently-Used.jpg)

### Advantages of LFU

-   **Adaptability:** Works well for varied access patterns.
-   **Optimized for Long-Term Trends:** Effective when long-term frequency matters more than recent accesses.
-   **Low Memory Overhead:** Avoids tracking timestamps, reducing memory use.

### Disadvantages of LFU

-   **Sensitivity to Initial Access:** May not perform well initially due to lack of frequency data.
-   **Difficulty Handling Changing Access Patterns:** Items that were once frequently accessed may persist unnecessarily.
-   **Complex Frequency Counting:** Requires tracking access counts for accurate eviction.

### Use Cases of LFU

-   **Database Query Caching:** Optimizes database performance by caching frequent queries.
-   **Network Routing Tables:** Caches frequently used network routes for efficient decisions.
-   **Content Recommendations:** Stores frequently accessed recommendations for users.

---

---

## 3. First-In-First-Out (FIFO)

First-In-First-Out (FIFO) is a cache eviction policy that removes the oldest item from the cache when it becomes full. In this strategy, data is stored in the cache in the order it arrives, and the item that has been present in the cache for the longest time is the first to be evicted when the cache reaches its capacity.

### Example:

Imagine a cache with a capacity of three items:

1. A is added to the cache.
2. B is added to the cache.
3. C is added to the cache.

![](https://media.geeksforgeeks.org/wp-content/uploads/20240110190121/First-In-First-Out.jpg)

At this point, the cache is full (capacity = 3). If a new item, D, needs to be added, the FIFO policy would dictate that the oldest item, A, should be evicted. The cache would then hold items B, C, and D.

### Advantages of FIFO

-   **Simple Implementation:** Easy to implement and understand.
-   **Predictable Behavior:** Eviction order follows time of entry.
-   **Memory Efficiency:** No need to track timestamps or access frequency.

### Disadvantages of FIFO

-   **Lack of Adaptability:** Doesn't consider access patterns or importance of items.
-   **Inefficiency with Variable Importance:** May evict frequently accessed items.
-   **Cold Start Issues:** Doesn't optimize based on usage, leading to inefficiencies.

### Use Cases of FIFO

-   **Task Scheduling in OS:** Determines execution order of processes.
-   **Message Queues:** Ensures messages are processed in arrival order.
-   **Streaming Applications:** Maintains proper frame order in video streams.

---

---

## 4. Random Replacement

Random Replacement is a cache eviction policy where, when the cache is full and a new item needs to be stored, a randomly chosen existing item is evicted to make room.

### Example:

Consider a cache with three slots:

-   Item A
-   Item B
-   Item C

If a new item D needs to be stored, Random Replacement might evict any item (e.g., Item B), resulting in:

-   Item A
-   Item D
-   Item C

![](https://media.geeksforgeeks.org/wp-content/uploads/20240110190210/Random-Replacement.jpg)

### Advantages of Random Replacement

-   **Simplicity:** Easy to implement with minimal tracking.
-   **Avoids Biases:** Doesn't rely on historical access patterns.
-   **Low Overhead:** Requires minimal computation.

### Disadvantages of Random Replacement

-   **Suboptimal Performance:** Doesn't optimize based on usage patterns.
-   **No Adaptability:** Doesn't adjust to changing access patterns.
-   **Poor Hit Rates:** May evict frequently accessed items unintentionally.

### Use Cases of Random Replacement

-   **Non-Critical Caching:** Suitable for temporary or non-essential data.
-   **Simulation & Testing:** Used in environments prioritizing simplicity.
-   **Resource-Constrained Systems:** Efficient for systems with limited computational resources.

---

## Conclusion

Cache eviction policies play a crucial role in system design, impacting efficiency and performance. The choice of a policy depends on system requirements. While simpler policies like Random Replacement offer ease of implementation, sophisticated strategies like LRU and LFU adapt better to changing workloads, optimizing cache performance over time.
