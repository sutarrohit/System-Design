# 1. Scalability vs. Performance

**Scalability** is about size: _"Can the system grow to handle more work?"_

**Performance** is about speed: _"How fast can the system complete a task?"_

They often pull in opposite directions, and improving one can sometimes impact the other.

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F277f4cdc-fb87-485b-901f-58da7ab90e1f_1184x504.png)

### Example

Adding more machines to a system can make it more scalable, but the complexity of managing these machines and coordinating tasks across them might introduce delays affecting performance.

Conversely, optimizing for performance with minimal resources may create scalability issues when user demand increases.

---

---

# 2. Vertical vs. Horizontal Scaling

**Vertical scaling** involves adding more resources to an existing server (e.g., CPU, RAM).  
**Horizontal scaling** means adding more servers to the pool.

-   Scaling **vertically** is simpler but has a physical limit on upgrades.

    -   It introduces a **single point of failure**â€”if the machine goes down, the entire system can become unavailable.

-   Scaling **horizontally** allows for almost limitless scaling but adds complexity in managing distributed systems.
    ![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F917f0e2c-1f90-4e95-aecf-9fd72ba5951b_2260x792.png)

### Example

Initially, a startup may **vertically scale** its server to handle increased load by adding more CPUs and RAM.

As the startup grows, it might shift to **horizontal scaling** by adding more servers to distribute the load.

---

---

# 3. **Latency vs. Throughput**

Latency and throughput are two fundamental performance metrics in system design, especially in distributed systems, networking, and databases.

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9f9305c7-7659-4e20-8551-c675f19e769f_1280x724.png)

### **1. Latency (Delay)**

-   **Definition**: The time taken to process a single request from start to finish.
-   **Measured in**: Milliseconds (ms) or seconds.
-   **Example**: If a user sends a request to a server and it takes 200ms to receive a response, the latency is **200ms**.
-   **Real-world analogy**: The time it takes for a message to reach a friend via a courier.

**Types of Latency:**

-   **Network Latency**: The time taken for data to travel across a network.
-   **Disk Latency**: The delay in reading/writing data from a disk.
-   **Processing Latency**: The time taken by a system to process a request.

---

### **2. Throughput (Capacity)**

-   **Definition**: The number of requests or operations a system can handle in a given time.
-   **Measured in**: Requests per second (RPS), transactions per second (TPS), or gigabits per second (Gbps).
-   **Example**: A web server handling **1,000 requests per second** has a throughput of **1,000 RPS**.
-   **Real-world analogy**: The number of messages a courier company can deliver per day.

---

### **Key Differences**

| Feature         | Latency                                        | Throughput                                               |
| --------------- | ---------------------------------------------- | -------------------------------------------------------- |
| **Definition**  | Time taken to process a single request         | Number of requests processed per second                  |
| **Measured in** | Milliseconds (ms), seconds                     | Requests per second (RPS), transactions per second (TPS) |
| **Focuses on**  | Speed of a single operation                    | System's capacity to handle multiple operations          |
| **Example**     | A database query taking 10ms to return results | A database handling 10,000 queries per second            |

---

### **Trade-off Between Latency and Throughput**

-   **Low Latency, High Throughput (Ideal Scenario)**: A well-optimized system minimizes delays while processing a large number of requests.
-   **High Throughput, High Latency**: If a system processes a lot of requests but each takes a long time, users may experience delays.
-   **Low Throughput, Low Latency**: If the system is fast but handles very few requests at a time, it may not scale well.

#### **Example in System Design:**

-   A **Content Delivery Network (CDN)** reduces latency by caching content closer to users.
-   A **Load Balancer** increases throughput by distributing traffic across multiple servers.

---

### **Conclusion**

-   **Latency** affects **individual request speed**.
-   **Throughput** affects **overall system capacity**.
-   **Optimizing both is crucial** for high-performance systems.

---

---

# 4. SQL vs. NoSQL Databases

### SQL (Relational Databases)

SQL databases are built on the **relational model**, organizing data into tables of rows and columns, with a **unique key** identifying each row.

-   **Highly structured** and offer **powerful query languages**, making them ideal for complex queries and transactions.
-   Examples: **MySQL, PostgreSQL**.
-   **Scaling horizontally** can be challenging.

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa81088b7-a188-4089-845a-63936e930a71_1632x1076.png)

#### Example

Banks use SQL databases for transaction management. These databases ensure that all transactions are processed reliably, maintaining accurate account balances and transaction histories.

### NoSQL (Non-Relational Databases)

NoSQL databases offer **flexibility** and scale easily but might sacrifice SQL-like query capabilities and ACID transactions.

-   Suited for **large volumes of unstructured or semi-structured data**.
-   Ideal for applications requiring **quick iterations and frequent code pushes**.
-   Examples: **Cassandra, Amazon DynamoDB**.

NoSQL databases come in various types, including:

-   **Document stores**
-   **Key-value stores**
-   **Wide-column stores**
-   **Graph databases**

#### Example

Companies like **Netflix** use NoSQL for real-time recommendation engines. NoSQL databases can quickly process large sets of diverse data (views, ratings, preferences) to deliver personalized content recommendations to millions of users.

---

---

# 5. Consistency vs. Availability (CAP Theorem)

**Consistency** means that every time someone accesses the system, they get the most recent data.

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40850582-9eaf-48e2-a254-43817c4d60c0_951x792.png)

### Example

In an e-commerce platform like **Amazon**, when a customer places an order, the system ensures that stock levels are **immediately updated**. This consistency prevents other customers from ordering the same item if it's unavailable.

**Availability** ensures the system is **always up and running**, even if some parts of it are experiencing issues.

### Example

In an online messaging service, availability ensures you can still **send and receive messages** even when some servers are down.

### CAP Theorem

In a **distributed system**, you can only guarantee **two out of the three**:

-   **Consistency (C)**
-   **Availability (A)**
-   **Partition Tolerance (P)**

---

---

# 6. Strong vs. Eventual Consistency

In distributed systems, where data is stored across multiple locations, ensuring that everyone sees the same data at the same time can be challenging.

This is where the concepts of **strong** and **eventual consistency** come into play.

### Strong Consistency

As soon as a data update occurs, any subsequent access to that data will reflect the update.

#### Example

In a **banking system**, when you transfer money from one account to another, the system updates the balances **immediately**.

### Eventual Consistency

There might be a **delay** before an update is visible across all nodes in a system. However, it is **guaranteed** that if no new updates are made, eventually, all accesses to that data will return the updated value.

#### Example

On a **social media platform** like **Instagram**, when you post a new photo, it might **not immediately appear** on all your followers' feeds. However, after a short period, everyone will see the latest updates.

---

---

# 7. Read-Through vs. Write-Through Cache

**Caching** is a technique used to speed up access to data by storing a copy of frequently accessed data in a faster storage medium.

When it comes to cache strategies, **Read-Through** and **Write-Through** are two common approaches.

### Read-Through Cache

A **read-through cache** checks the cache first when data is requested. If the data isn't there (**cache miss**), it's loaded from the slower **primary storage** into the cache before being returned.

-   Useful for **read-heavy applications** where data is frequently accessed but not often updated.
-   Reduces load on the primary storage and improves **read performance**.

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9ed5a614-e276-4ce6-b495-a8867393ba56_1774x978.png)

#### Example

In an **online store**, when a customer views a product for the first time, the product details are **fetched from the database** and **stored in the cache**. Subsequent views are **served from the cache**, speeding up response time and reducing database queries.

### Write-Through Cache

A **write-through cache** simultaneously writes **data updates** to both the **cache** and **primary storage**, ensuring **up-to-date data** and reducing data loss risks.

-   Beneficial for **write-heavy applications**.
-   All writes are instantly reflected in both the cache and primary storage.

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4098acb3-2870-498b-bdc2-cd70cdf5c205_1772x980.png)

#### Example

A **movie ticket booking system** uses a **write-through cache** to prevent **overbooking** by **immediately recording bookings** both in the **cache** and the **database**.

---

---

# 8. Batch vs. Stream Processing

**Batch processing** involves collecting data over a period of time and then processing it all at once.

**Stream processing**, on the other hand, deals with data in **real-time**, processing it as soon as it arrives.

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff898c15e-8bc2-4fc7-aa07-e4d95bc93dc2_1512x812.png)

### Example

-   **Batch Processing:** Credit card companies use batch processing for **daily billing** and **statement generation**.
-   **Stream Processing:** For **fraud detection**, they implement stream processing to **analyze transactions in real-time** and flag suspicious activities immediately.

---

---

# 9. Synchronous vs. Asynchronous Processing

**Synchronous processing** means tasks are performed **one after another**. A task must be completed before the next one starts, and the system **waits for the outcome** before proceeding.

### Example

When you make an **online purchase**, the **payment process** is synchronous. After you click _"Pay Now,"_ you **wait** for the transaction to process. The website **does not let you move forward** until it confirms that your payment was successful.

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ba911a5-4116-45d5-9ff1-324355f8ebf9_1452x908.png)

**Asynchronous processing** allows tasks to **run in the background** and doesn't need to wait for completion before starting another task.

### Example

Uploading **photos on social media** happens **asynchronously** in the background. You can **keep scrolling** or **exit the app** while the photo uploads.

---

---

## Stateful vs. Stateless Architecture

A **stateful system** remembers past interactions. It stores information about the current session, allowing **continuity and context** in subsequent interactions without needing to start from scratch each time.

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce6d4789-603d-4422-b3e7-31c409a83b72_1920x1286.png)

### Example

During **online shopping**, when you add items to your **cart**, the website **remembers your selections**. If you navigate away to browse more items and then return to your cart, your items **are still there**, waiting for you to check out.

A **stateless system** does **not** keep track of past interactions. Each request is treated as new, with **no information retained** from previous requests.

### Example

Many **RESTful web services** operate without remembering past requests. For instance, when you **query a public API** for **weather information**, you must **provide all necessary details** (like location) with **each request**.

---

---

# 11. Long Polling vs. WebSockets

### Long Polling

Long polling is a technique where the **client requests data** from the server, and the **server holds the request open** until new information is available. After receiving the data, the client **instantly sends a new request**, enabling immediate updates.

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe2ca9ebb-e111-4a9b-87db-2510027b2576_1554x960.png)

#### Example

A **social media platform's notification system**: The browser continuously queries the server for new notifications, and the server responds **when a new notification appears** or a timeout occurs.

### WebSockets

WebSockets provide a **full-duplex communication channel** over a **single, long-lived connection**, allowing the **server and client to send data back and forth** as soon as it's available, without waiting for a request.

#### Example

In a **multiplayer online game**, WebSockets enable **real-time sharing** of player actions and game updates, thanks to the **persistent connection** between client and server.

---

---

# 12. Normalization vs. Denormalization

### Normalization

**Normalization** in database design involves **splitting up data** into related tables to ensure each piece of information is **stored only once**.

-   Reduces **redundancy** and improves **data integrity**.

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F075a3c18-516e-48d5-b703-156b580adc97_1198x1224.png)

#### Example

A **customer database** can have two separate tables:

-   One for **customer details**
-   Another for **orders**

This avoids **duplication** of customer information for each order.

### Denormalization

**Denormalization** is the process of **combining data** back into **fewer tables** to improve **query performance**. This often introduces **redundancy** (duplicate information) back into the database.

#### Example

A **blog website** can store the **latest comments** within the same table as posts (**denormalized**) to **speed up the display** of posts and comments, instead of storing them separately (**normalized**).

---

---

# 13. Monolithic vs. Microservices Architecture

### Monolithic Architecture

A **monolithic architecture** is built as a **single unified unit**, where all parts of an application are **bundled together**.

#### Pros

-   **Simple** and **easy to deploy**
-   Suitable for **smaller applications or teams**

#### Cons

-   **Slows down development** as the application grows
-   **Complicates scalability**

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98ba8e4b-dfe8-435e-8124-ddc306917e9a_1334x780.png)

---

### Microservices Architecture

A **microservices architecture** is a collection of **smaller, independently deployable services**.

#### Pros

-   **Improves scalability**
-   **Increases development velocity**

#### Cons

-   **Introduces complexity** in service management and data consistency
-   Can **increase communication overhead**

### Example

A **small web application** might start as a **monolith** for simplicity. As it grows, it could evolve into a **microservices architecture**, splitting into **smaller, independently scalable services** for better agility and scalability.

---

---

# 14. REST vs. GraphQL

### REST

**REST** is a well-established standard for APIs, offering **simplicity** and support for **multiple formats**.

-   Requires accessing **multiple endpoints** to gather related data.

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7273bd51-a878-4ba0-b26e-a6738a445d6f_1830x1050.png)

### GraphQL

**GraphQL** provides **more efficient data fetching** with **fewer requests**, but it has a **steeper learning curve** and requires more upfront design.

-   In GraphQL, a **single query** is sent to the server, specifying the **exact data requirements**.
-   The server **responds with a JSON object** containing only the requested data.

---

---

# 15. TCP vs. UDP

**TCP** and **UDP** are communication standards for delivering data and messages through networks.

### TCP (Transmission Control Protocol)

-   Ensures **reliable delivery** of data in the **exact order** it was sent.
-   Establishes a **connection** between sender and receiver.
-   **Checks for errors** and **resends lost data**.
-   **Ideal for:** Applications where reliability is crucial, such as **email services** and **file transfers**.

### UDP (User Datagram Protocol)

-   Sacrifices **reliability** for **speed**.
-   Sends data **without establishing a connection**.
-   No guarantee that data is **received or in order**.
-   **Ideal for:** Time-sensitive applications like **video streaming** and **online gaming**, where lower latency is more important than perfect data delivery.
