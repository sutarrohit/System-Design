# **Messaging Pattern**

The **Messaging** pattern is used for **communication and coordination** between different components or systems using **message queues, brokers, or event buses**. It helps in **decoupling** the sender and receiver, making the system more **scalable, flexible, and fault-tolerant**.

---

### **How It Works**

1. **Producer (Sender)** – Sends a message to a **message broker** instead of directly communicating with a receiver.
2. **Message Broker** – Stores and forwards messages to the appropriate consumer(s). Examples include **RabbitMQ, Kafka, AWS SQS, and Redis Pub/Sub**.
3. **Consumer (Receiver)** – Listens for messages and processes them asynchronously.

---

### **Example Use Cases**

📌 **Order Processing in E-commerce** – When a user places an order, a message is sent to a queue, and the order service processes it asynchronously.  
📌 **Real-time Notifications** – A messaging system can send notifications to users (e.g., in a chat app).  
📌 **Log Aggregation & Monitoring** – Collecting logs from different services and processing them using Kafka or RabbitMQ.  
📌 **IoT Applications** – Devices publish sensor data to a message queue, and multiple services consume it.

---

### **Real-World Example (RabbitMQ + Node.js)**

🔥 **Producer (Sends Messages to RabbitMQ Queue)**

```javascript
const amqp = require("amqplib");

async function sendMessage() {
    const connection = await amqp.connect("amqp://localhost");
    const channel = await connection.createChannel();

    const queue = "task_queue";
    const message = "Hello, Messaging Pattern!";

    await channel.assertQueue(queue, { durable: true });
    channel.sendToQueue(queue, Buffer.from(message));

    console.log(`Sent: ${message}`);
    await channel.close();
    await connection.close();
}

sendMessage();
```

🔥 **Consumer (Receives & Processes Messages)**

```javascript
const amqp = require("amqplib");

async function receiveMessage() {
    const connection = await amqp.connect("amqp://localhost");
    const channel = await connection.createChannel();

    const queue = "task_queue";

    await channel.assertQueue(queue, { durable: true });
    console.log(`Waiting for messages in ${queue}...`);

    channel.consume(queue, (msg) => {
        console.log(`Received: ${msg.content.toString()}`);
        channel.ack(msg);
    });
}

receiveMessage();
```

---

### **Potential Downsides**

⚠️ **Complexity** – Requires additional infrastructure (e.g., RabbitMQ, Kafka).  
⚠️ **Message Ordering** – Ensuring messages are processed in the right order can be challenging.  
⚠️ **Latency** – Messages might not be processed instantly, which may not be ideal for real-time systems.

---

---

# 1. **Sequential Convoy Pattern**

The **Sequential Convoy** pattern ensures that a **series of dependent tasks** (a "convoy") execute in a **specific order**, maintaining correctness and handling failures. This pattern is useful for **workflows, transactions, and stateful processing** where steps must follow a defined sequence.

---

### **How It Works**

1. **Tasks are executed in a strict sequence** – Each step depends on the successful completion of the previous one.
2. **Error handling mechanisms** ensure that failures do not corrupt the process (e.g., retries, compensating transactions).
3. **Technologies like state machines, workflow engines, and distributed transactions** can help implement this pattern.

---

### **Implementation Approaches**

#### 🔹 **Using a State Machine (AWS Step Functions Example)**

AWS Step Functions can define **ordered workflows** with error handling.  
Example JSON for an order processing workflow:

```json
{
    "StartAt": "Validate Payment",
    "States": {
        "Validate Payment": {
            "Type": "Task",
            "Next": "Deduct Inventory"
        },
        "Deduct Inventory": {
            "Type": "Task",
            "Next": "Generate Invoice"
        },
        "Generate Invoice": {
            "Type": "Task",
            "Next": "Ship Order"
        },
        "Ship Order": {
            "Type": "Task",
            "End": true
        }
    }
}
```

AWS Step Functions ensure **sequential execution** with built-in retry policies.

---

#### 🔹 **Using Kafka for Ordered Processing**

Kafka **ensures message order within a partition**, making it suitable for sequential task execution.

```javascript
const { Kafka } = require("kafkajs");

const kafka = new Kafka({ clientId: "convoy", brokers: ["localhost:9092"] });
const producer = kafka.producer();
const consumer = kafka.consumer({ groupId: "order-group" });

async function run() {
    await producer.connect();
    await consumer.connect();
    await consumer.subscribe({ topic: "order-topic", fromBeginning: true });

    consumer.run({
        eachMessage: async ({ message }) => {
            console.log(`Processing step: ${message.value.toString()}`);
            // Perform each task in sequence
        }
    });

    // Send messages in sequence
    await producer.send({
        topic: "order-topic",
        messages: [{ value: "Validate Payment" }, { value: "Deduct Inventory" }, { value: "Generate Invoice" }, { value: "Ship Order" }]
    });
}

run();
```

Kafka ensures ordered execution by **preserving message order in partitions**.

---

---

# 2. **Scheduling Agent Supervisor Pattern**

The **Scheduling Agent Supervisor** pattern ensures that a **set of distributed actions** is coordinated as a **single operation**. If any action **fails**, the system attempts to **retry, recover, or roll back** the operation to maintain consistency. This pattern is particularly useful in **distributed systems**, improving **resiliency and fault tolerance**.

---

### **How It Works**

1. **Agent Schedules & Executes Tasks** – A scheduling agent triggers a set of actions in different services.
2. **Monitoring & Recovery** – If an action **fails**, the supervisor retries or applies **compensating actions** (e.g., rollback).
3. **Final Success or Failure** – If all tasks succeed, the operation is considered **successful**; otherwise, it **fails as a whole**.

---

### **Example Use Cases**

📌 **Payment Processing** – Deduct money, update inventory, and confirm order as a single operation.  
📌 **Microservices Coordination** – Ensuring all services complete their part of a transaction (or rollback).  
📌 **Cloud Resource Allocation** – Ensuring VMs, storage, and databases are provisioned together.  
📌 **CI/CD Pipelines** – Running multiple deployment steps and rolling back on failure.

---

### **Implementation Approaches**

#### 🔹 **Using a Workflow Engine (Temporal.io Example)**

Temporal.io allows defining workflows that ensure all steps are completed or rolled back.  
Example: **E-commerce Order Processing Workflow**

```typescript
import { workflow, activity } from "@temporalio/workflow";

// Define steps as activities
const validatePayment = activity("validatePayment");
const reserveInventory = activity("reserveInventory");
const generateInvoice = activity("generateInvoice");
const rollback = activity("rollback");

export const orderWorkflow = workflow.define({
    async execute(orderId: string) {
        try {
            await validatePayment(orderId);
            await reserveInventory(orderId);
            await generateInvoice(orderId);
        } catch (error) {
            console.error("Order processing failed, rolling back...");
            await rollback(orderId); // Undo any completed actions
            throw error; // Ensure workflow fails as a whole
        }
    }
});
```

✅ If **any step fails**, the workflow **rolls back previous actions** and ensures the operation doesn't leave the system in an inconsistent state.

---

---

# 3. **Publisher-Subscriber Pattern (Pub-Sub)**

The **Publisher-Subscriber (Pub-Sub) pattern** enables an application to **announce events asynchronously** to multiple subscribers **without being directly coupled** to them. This pattern is commonly used in **event-driven architectures**, microservices, and real-time systems.

---

### **How It Works**

1. **Publisher (Producer)** – Generates and **publishes events/messages**.
2. **Message Broker** – A middleware component (e.g., Kafka, RabbitMQ, Redis Pub/Sub) **routes events** to subscribers.
3. **Subscribers (Consumers)** – **Listen** for and process **specific events**.

✅ **Loose Coupling** – Publishers don’t need to know who the subscribers are.  
✅ **Scalability** – Multiple subscribers can process the same event in parallel.  
✅ **Asynchronous Processing** – Subscribers process events **independently and at their own pace**.

---

### **Example Use Cases**

📌 **Microservices Communication** – Services notify others about changes (e.g., Order Placed → Notify Inventory, Shipping).  
📌 **Event-Driven Architecture** – Decoupled services react to events (e.g., User Signup → Send Welcome Email).  
📌 **Real-time Notifications** – Chat apps, stock price updates, social media feeds.  
📌 **Logging & Monitoring** – Collecting and analyzing logs from multiple sources.

---

### **Implementation Approaches**

#### 🔹 **Using Redis Pub/Sub** (Simple and lightweight)

```javascript
const redis = require("redis");

// Publisher
const pub = redis.createClient();
pub.publish("orderChannel", JSON.stringify({ orderId: 123, status: "placed" }));

// Subscriber
const sub = redis.createClient();
sub.subscribe("orderChannel");

sub.on("message", (channel, message) => {
    console.log(`Received on ${channel}: ${message}`);
});
```

✅ Simple, real-time messaging  
❌ No message persistence – If a subscriber is offline, it **misses messages**

---

#### 🔹 **Using Kafka (Distributed & Persistent Messaging)**

**Kafka** ensures messages are **stored and replayable**, making it great for scalable systems.

```javascript
const { Kafka } = require("kafkajs");

// Create Kafka instance
const kafka = new Kafka({ clientId: "pubsub", brokers: ["localhost:9092"] });

// Publisher (Producer)
const producer = kafka.producer();
await producer.connect();
await producer.send({
    topic: "order-events",
    messages: [{ value: JSON.stringify({ orderId: 123, status: "placed" }) }]
});
await producer.disconnect();

// Subscriber (Consumer)
const consumer = kafka.consumer({ groupId: "order-group" });
await consumer.connect();
await consumer.subscribe({ topic: "order-events", fromBeginning: true });

await consumer.run({
    eachMessage: async ({ message }) => {
        console.log(`Received: ${message.value.toString()}`);
    }
});
```

✅ **Scalable & Fault-Tolerant** – Messages persist, can replay old events.  
✅ **Supports Multiple Subscribers** – Each service can consume messages **independently**.

---

---

# 4. **Priority Queue Pattern**

The **Priority Queue pattern** ensures that **higher-priority requests** are **processed first**, while **lower-priority requests** wait in the queue. This is useful in applications that need to enforce **Service Level Agreements (SLAs)** or **prioritize critical tasks** over less urgent ones.

### **How It Works**

1. **Clients send requests** with an associated **priority level**.
2. **A priority queue** (in-memory or message broker-based) **orders requests** based on priority.
3. **Workers (consumers)** process high-priority requests first.

---

### **Example Use Cases**

📌 **Customer Support Systems** – Premium users get faster responses than free-tier users.  
📌 **Cloud Resource Allocation** – Urgent requests for computing power get processed first.  
📌 **Real-Time Processing** – Financial transactions, emergency alerts, or fraud detection.  
📌 **Task Scheduling** – Background jobs with different urgencies (e.g., sending OTP vs. generating reports).

---

### **Implementation Approaches**

#### 🔹 **Using a Priority Queue with Redis**

Redis **Sorted Sets** can store messages with a **priority score**.

```javascript
const redis = require("redis");
const client = redis.createClient();

// Add tasks with priority (lower score = higher priority)
client.zadd("priorityQueue", 1, "urgent-task"); // High priority
client.zadd("priorityQueue", 5, "normal-task"); // Low priority

// Fetch and process the highest-priority task
client.zpopmin("priorityQueue", (err, task) => {
    console.log(`Processing: ${task[0]}`);
});
```

✅ **Fast & Efficient** (O(log n) retrieval)  
❌ **No Persistence** – If Redis restarts, queued messages are lost

---

#### 🔹 **Using RabbitMQ with Priority Queues**

RabbitMQ supports **message prioritization** using **priority queues**.

```javascript
const amqp = require("amqplib");

async function sendMessage(priority, message) {
    const connection = await amqp.connect("amqp://localhost");
    const channel = await connection.createChannel();

    const queue = "taskQueue";
    await channel.assertQueue(queue, { durable: true, maxPriority: 10 });

    channel.sendToQueue(queue, Buffer.from(message), { priority });
    console.log(`Sent: ${message} (Priority: ${priority})`);

    await channel.close();
    await connection.close();
}

// Sending tasks with different priorities
sendMessage(10, "High-priority task"); // Highest priority
sendMessage(1, "Low-priority task"); // Lowest priority
```

✅ **Supports Persistent Messaging** – Messages remain even after RabbitMQ restarts  
✅ **Scalable** – Multiple workers can process tasks in priority order

---

#### 🔹 **Using AWS SQS with FIFO + Message Attributes**

AWS **Simple Queue Service (SQS)** supports FIFO queues, and priority can be implemented using **message attributes** (e.g., `PriorityLevel`).

```json
{
    "MessageBody": "Process Order",
    "MessageAttributes": {
        "Priority": { "DataType": "Number", "StringValue": "1" }
    }
}
```

✅ **Cloud-native solution**  
✅ **Highly Available & Scalable**

---

---

# 5. **Pipes and Filters Pattern**

The **Pipes and Filters pattern** decomposes a **complex processing task** into a **series of independent steps** (filters), where the **output of one step becomes the input for the next**. This pattern improves **scalability, reusability, and maintainability**.

### **How It Works**

1. **Filters** – Each filter processes data independently and passes it to the next stage.
2. **Pipes** – Pipes act as connectors between filters, ensuring smooth data flow.

---

### **Example Use Cases**

📌 **Data Processing Pipelines** – ETL (Extract, Transform, Load) in big data systems.  
📌 **Streaming Applications** – Video/audio processing pipelines.  
📌 **Microservices Architectures** – Chaining independent services together.  
📌 **Machine Learning Pipelines** – Preprocessing, training, and inference stages.

---

### **Implementation Approaches**

#### 🔹 **Using Unix/Linux Pipes**

In Unix systems, commands are chained using `|` (pipes).

```sh
cat data.txt | grep "error" | sort | uniq > filtered_errors.txt
```

✅ **Simple & Efficient** – Works well for text processing.  
❌ **Limited to CLI-based workflows**

---

#### 🔹 **Using JavaScript Functions (Chaining Functions as Filters)**

```javascript
// Define filters (processing functions)
const toUpperCase = (text) => text.toUpperCase();
const removeSpaces = (text) => text.replace(/\s+/g, "");
const reverseText = (text) => text.split("").reverse().join("");

// Pipe function to chain filters
const pipe =
    (...functions) =>
    (input) =>
        functions.reduce((acc, fn) => fn(acc), input);

// Create a processing pipeline
const textPipeline = pipe(toUpperCase, removeSpaces, reverseText);

// Apply the pipeline
console.log(textPipeline("hello world")); // Output: "DLROWOLLEH"
```

✅ **Reusable and Modular** – Each function is independent.  
✅ **Flexible** – New filters can be added easily.

---

#### 🔹 **Using Node.js Streams (Efficient Data Processing)**

```javascript
const fs = require("fs");
const { Transform } = require("stream");

// Create a transformation filter (stream)
const uppercaseFilter = new Transform({
    transform(chunk, encoding, callback) {
        this.push(chunk.toString().toUpperCase());
        callback();
    }
});

// Pipe data through filters
fs.createReadStream("input.txt")
    .pipe(uppercaseFilter) // Convert to uppercase
    .pipe(fs.createWriteStream("output.txt")); // Save to file
```

✅ **Handles Large Data Efficiently** – Works with streams (no need to load everything in memory).  
✅ **Optimized for Performance** – Data flows through filters incrementally.

---

#### 🔹 **Using Kafka for Distributed Processing Pipelines**

Kafka topics act as pipes, and microservices (consumers) process data at each step.

```json
{
    "input_topic": "raw_orders",
    "filters": [
        { "name": "validateOrder", "output_topic": "validated_orders" },
        { "name": "applyDiscount", "output_topic": "discounted_orders" },
        { "name": "finalizeOrder", "output_topic": "final_orders" }
    ]
}
```

✅ **Scalable & Distributed** – Each filter can be a microservice.  
✅ **Fault-Tolerant** – Kafka ensures message persistence and retries.

---

---

# 6. **Competing Consumers Pattern**

The **Competing Consumers pattern** enables **multiple consumers to concurrently process messages** from the **same message queue**, optimizing **throughput, scalability, and fault tolerance**.

### **How It Works**

1. **Producers** send messages to a **message queue**.
2. **Multiple consumers** (workers) pull messages **independently** from the queue.
3. Each message is processed **only once** by a single consumer.
4. If a consumer fails, **another consumer picks up the unprocessed message**.

---

### **Example Use Cases**

📌 **Order Processing** – Handle multiple e-commerce orders in parallel.  
📌 **Background Jobs** – Process large batches of tasks efficiently.  
📌 **Log Aggregation** – Distribute log analysis across multiple workers.  
📌 **Fraud Detection** – Analyze multiple transactions simultaneously.

---

### **Implementation Approaches**

#### 🔹 **Using RabbitMQ (AMQP Message Broker)**

RabbitMQ enables **multiple consumers to process tasks from a queue**.

```javascript
const amqp = require("amqplib");

async function consumeMessages() {
    const connection = await amqp.connect("amqp://localhost");
    const channel = await connection.createChannel();

    const queue = "taskQueue";
    await channel.assertQueue(queue, { durable: true });

    console.log("Waiting for messages...");

    channel.consume(queue, (msg) => {
        console.log(`Processing: ${msg.content.toString()}`);
        channel.ack(msg); // Acknowledge message after processing
    });
}

consumeMessages();
```

✅ **Automatic Load Balancing** – Messages are **evenly distributed** among consumers.  
✅ **Fault-Tolerant** – If a worker crashes, unprocessed messages remain in the queue.

---

#### 🔹 **Using AWS SQS (Serverless Competing Consumers)**

AWS **Simple Queue Service (SQS)** allows **multiple consumers (Lambda, EC2, Fargate) to pull messages**.

1. **Producers** send messages to the **SQS queue**.
2. **Multiple consumers (workers)** poll the queue to process messages.
3. **If a worker fails, the message remains in the queue** and is retried.

**Example Message in AWS SQS:**

```json
{
    "MessageBody": "Process Order #12345",
    "MessageAttributes": {
        "Priority": { "DataType": "Number", "StringValue": "1" }
    }
}
```

✅ **Auto-scaling with AWS Lambda** – Consumers scale automatically.  
✅ **Fault-tolerant** – Messages are retried until successfully processed.

---

#### 🔹 **Using Apache Kafka for Competing Consumers**

Kafka **consumer groups** allow **multiple consumers to process messages from a topic**.

```javascript
const { Kafka } = require("kafkajs");

const kafka = new Kafka({ clientId: "my-app", brokers: ["localhost:9092"] });

async function consumeMessages() {
    const consumer = kafka.consumer({ groupId: "order-processors" });
    await consumer.connect();
    await consumer.subscribe({ topic: "orders", fromBeginning: true });

    await consumer.run({
        eachMessage: async ({ message }) => {
            console.log(`Processing order: ${message.value.toString()}`);
        }
    });
}

consumeMessages();
```

✅ **Scalable Consumers** – More workers can be added dynamically.  
✅ **Message Distribution** – Kafka assigns partitions to consumers for **load balancing**.

---

---

# 7. **Choreography Pattern**

The **Choreography pattern** allows **multiple components in a distributed system to make decisions and coordinate workflows independently**, **without relying on a central orchestrator**. Each component **reacts to events** and determines its next action, leading to a **loosely coupled, scalable, and resilient system**.

### **How It Works**

1. **Event-Driven** – Services communicate through **events**, not direct API calls.
2. **Decentralized Control** – Each service **decides its own actions** based on events.
3. **Asynchronous Execution** – Services process tasks **independently**, improving scalability.
4. **Reactive Communication** – New services can **subscribe to relevant events** without modifying existing services.

---

### **Example Use Cases**

📌 **E-commerce Order Processing** – Payment, inventory, and shipping services act independently.  
📌 **Microservices Workflows** – Services respond to **domain events** without a central controller.  
📌 **Event-Driven Systems** – Real-time analytics, fraud detection, IoT processing.  
📌 **Serverless Workflows** – AWS Lambda, Google Cloud Functions reacting to events.

---

### **Implementation Approaches**

#### 🔹 **Example: E-commerce Order Processing with Choreography**

When a customer places an order, multiple services react **without central control**.

1️⃣ **Order Service**: Creates an order and publishes an `OrderPlaced` event.  
2️⃣ **Payment Service**: Listens for `OrderPlaced`, processes payment, and emits `PaymentCompleted`.  
3️⃣ **Inventory Service**: Listens for `PaymentCompleted`, reserves stock, and emits `StockReserved`.  
4️⃣ **Shipping Service**: Listens for `StockReserved`, generates tracking info, and emits `OrderShipped`.

**Message Broker (e.g., Kafka, RabbitMQ, AWS SNS/SQS) handles event communication.**

✅ **No direct dependencies between services**  
✅ **Each service reacts independently**  
✅ **New services can be added without modifying existing ones**

---

#### 🔹 **Using Kafka for Event-Driven Choreography**

```javascript
const { Kafka } = require("kafkajs");

const kafka = new Kafka({ clientId: "order-service", brokers: ["localhost:9092"] });

async function processOrder(order) {
    const producer = kafka.producer();
    await producer.connect();
    await producer.send({
        topic: "order_placed",
        messages: [{ value: JSON.stringify(order) }]
    });
    console.log("Order placed event published");
    await producer.disconnect();
}

processOrder({ orderId: 12345, user: "John Doe", amount: 100 });
```

✅ **Asynchronous and Event-Driven**  
✅ **Scales easily** – More consumers can be added as needed.

---

---

# 8. **Claim Check Pattern**

The **Claim Check pattern** is used to **handle large messages efficiently in a distributed system**. Instead of passing a large payload through the messaging system, the actual data is stored in an external **data store (e.g., S3, database, cache)**, and a **reference (claim check)** is sent through the message bus. Consumers use the claim check to **retrieve the full payload when needed**.

### **How It Works**

1️⃣ **Producer**:

-   Stores the **large payload** in an **external storage** (e.g., S3, database, cache).
-   Generates a **claim check (reference ID or URL)** for the stored payload.
-   Sends the **claim check** through the **message queue** instead of the actual payload.

2️⃣ **Consumer**:

-   Receives the **claim check** from the message queue.
-   Uses the **claim check** to fetch the full payload from the external store.
-   Processes the retrieved data.

---

### **Example Use Cases**

📌 **Image & Video Processing** – Instead of sending a large image via a queue, send a **URL reference**.  
📌 **Large File Transfers** – Store logs, reports, or documents in a cloud store (e.g., AWS S3) and send a **reference**.  
📌 **Big Data Processing Pipelines** – Pass dataset **identifiers** instead of full data payloads.  
📌 **IoT & Sensor Data** – Reduce message size by sending only a **claim check for raw data** stored in a database.

---

### **Implementation Approaches**

#### 🔹 **Example: Using AWS S3 & SQS for Claim Check**

✅ **AWS S3** – Stores large payloads.  
✅ **AWS SQS** – Sends claim checks (message references).

**Producer (Uploads data & sends claim check)**

```javascript
const AWS = require("aws-sdk");
const s3 = new AWS.S3();
const sqs = new AWS.SQS();

async function sendMessage(payload) {
    const bucketName = "my-large-data-store";
    const key = `payloads/${Date.now()}.json`;

    // Upload large payload to S3
    await s3
        .putObject({
            Bucket: bucketName,
            Key: key,
            Body: JSON.stringify(payload)
        })
        .promise();

    // Send claim check (reference) to SQS
    await sqs
        .sendMessage({
            QueueUrl: "https://sqs.us-east-1.amazonaws.com/123456789012/MyQueue",
            MessageBody: JSON.stringify({ reference: key })
        })
        .promise();

    console.log(`Sent claim check: ${key}`);
}

sendMessage({ userId: 123, transaction: "large-data-content" });
```

**Consumer (Retrieves data using the claim check)**

```javascript
async function processMessage(message) {
    const reference = JSON.parse(message.Body).reference;

    // Fetch full payload from S3
    const data = await s3
        .getObject({
            Bucket: "my-large-data-store",
            Key: reference
        })
        .promise();

    console.log("Retrieved payload:", data.Body.toString());
}

processMessage({ Body: JSON.stringify({ reference: "payloads/1701234567890.json" }) });
```

✅ **Minimizes message size in SQS**  
✅ **Ensures large payloads don’t overwhelm the message queue**

---

---

### **Asynchronous Request-Reply Pattern**

The **Asynchronous Request-Reply** pattern helps **decouple** backend processing from the frontend when **responses take time to generate**. It enables a frontend to send a request, continue working **without blocking**, and later retrieve the result once it's ready.

---

# 9. **How It Works**

1️⃣ **Frontend sends a request** to a backend service and receives an **immediate acknowledgment** (request ID or reference).  
2️⃣ **Backend processes the request asynchronously** in the background (e.g., long-running tasks like video processing, ML inference).  
3️⃣ **Frontend periodically checks for a response** (polling) or receives a notification when the task is complete (callback/webhook).  
4️⃣ **Once processing is done**, the backend sends back the **final result**.

### **Example Use Cases**

📌 **Video Processing** – Users upload videos, and the frontend checks processing status.  
📌 **Machine Learning Inference** – Model training or large dataset processing takes time.  
📌 **Order Processing** – E-commerce checkout with payment confirmation.  
📌 **Bank Transactions** – Money transfers take time but users receive a status update later.

---

### **Implementation Approaches**

#### 🔹 **1. Polling (Frontend Keeps Checking for Status)**

-   **Frontend makes a request** → Gets a job ID.
-   **Backend processes asynchronously**.
-   **Frontend periodically polls** the backend for updates.

✅ Simple to implement  
❌ Inefficient (wastes requests if the job isn't done yet)

**Example: Polling with Express & Node.js**

```javascript
const express = require("express");
const app = express();

let taskResults = {}; // Store task statuses

// Step 1: Receive request, assign a task ID, and return immediately
app.post("/start-task", (req, res) => {
    const taskId = Date.now();
    taskResults[taskId] = { status: "Processing", result: null };

    // Simulate async processing
    setTimeout(() => {
        taskResults[taskId] = { status: "Completed", result: "Task finished!" };
    }, 5000);

    res.json({ taskId });
});

// Step 2: Polling endpoint to check task status
app.get("/task-status/:taskId", (req, res) => {
    const taskId = req.params.taskId;
    res.json(taskResults[taskId] || { status: "Unknown Task" });
});

app.listen(3000, () => console.log("Server running on port 3000"));
```

✅ **Works well for simple use cases**  
❌ **Inefficient for high-frequency polling**

---

#### 🔹 **2. Webhook (Backend Notifies When Done)**

-   **Frontend sends a request with a callback URL**.
-   **Backend processes the task asynchronously**.
-   **Backend calls the provided callback URL when processing is complete**.

✅ No unnecessary polling  
✅ Reduces load on frontend  
❌ Requires exposing an API for callbacks

Example:  
📌 **Webhook for payment processing:**

1. User requests a transaction → Backend starts processing.
2. Once completed, the backend **calls the frontend’s webhook URL** to send the response.

---

#### 🔹 **3. Message Queue (Event-Driven Approach)**

-   **Frontend pushes a request** to a message queue (e.g., RabbitMQ, Kafka, AWS SQS).
-   **Backend workers pick up the job asynchronously**.
-   **Frontend checks status or gets notified via WebSocket or event streaming**.

✅ Best for **scalability** and **high loads**  
✅ Handles failures gracefully  
❌ Requires additional infrastructure (message queues, WebSockets, etc.)

Example: **Using Redis Queue for Background Jobs**

```javascript
const Queue = require("bull");
const myQueue = new Queue("tasks");

app.post("/start-task", async (req, res) => {
    const job = await myQueue.add({ data: "process this" });
    res.json({ jobId: job.id });
});

app.get("/task-status/:jobId", async (req, res) => {
    const job = await myQueue.getJob(req.params.jobId);
    res.json({ status: job ? job.state : "Unknown Task" });
});
```

✅ Efficient for large-scale applications  
✅ Works well with **serverless architectures**

---

---

# 10. **Queue-Based Load Leveling Pattern**

The **Queue-Based Load Leveling** pattern is used to handle **high traffic and sudden spikes in requests** by placing a **queue** between a sender (producer) and a receiver (consumer). This pattern helps distribute workloads **evenly over time**, ensuring that backend services are not overwhelmed.

### **How It Works**

🔹 **Producers (Senders)** send requests to a **queue** instead of directly processing them.  
🔹 **Consumers (Workers)** pull messages from the queue and process them at a steady rate.  
🔹 If the system experiences a **spike in traffic**, the queue **buffers** the extra load.  
🔹 Consumers can scale **horizontally** (adding more workers) to handle more requests.

---

### **Implementation Approaches**

#### **🔹 1. Using Redis as a Message Queue**

```javascript
const Redis = require("ioredis");
const redis = new Redis();

// Producer: Adding tasks to the queue
app.post("/add-task", async (req, res) => {
    const taskData = { id: Date.now(), action: "process video" };
    await redis.lpush("taskQueue", JSON.stringify(taskData));
    res.json({ message: "Task added to queue", taskData });
});

// Consumer: Processing tasks from the queue
setInterval(async () => {
    const task = await redis.rpop("taskQueue");
    if (task) {
        console.log("Processing task:", JSON.parse(task));
    }
}, 1000);
```

✅ **Simple and fast**  
❌ **Not ideal for large-scale workloads**

---

#### **🔹 2. Using AWS SQS for Scalable Queue Processing**

📌 **AWS Simple Queue Service (SQS)** can handle millions of requests without crashing.

1️⃣ **Producer**: Frontend sends a message to **SQS queue**  
2️⃣ **Queue**: Stores and manages messages  
3️⃣ **Consumer**: Backend service pulls messages for processing

```javascript
const AWS = require("aws-sdk");
const sqs = new AWS.SQS({ region: "us-east-1" });

// Producer: Send message to SQS
async function sendMessage() {
    const params = {
        QueueUrl: "https://sqs.us-east-1.amazonaws.com/your-queue-url",
        MessageBody: JSON.stringify({ orderId: 123, status: "new" })
    };
    await sqs.sendMessage(params).promise();
}

// Consumer: Receive message from SQS
async function processMessage() {
    const params = { QueueUrl: "https://sqs.us-east-1.amazonaws.com/your-queue-url" };
    const messages = await sqs.receiveMessage(params).promise();
    if (messages.Messages) {
        console.log("Processing:", JSON.parse(messages.Messages[0].Body));
    }
}
```

✅ **Scales automatically**  
✅ **Supports distributed processing**  
❌ **Higher latency than in-memory queues**

---
